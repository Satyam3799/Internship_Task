
# ðŸš¢ Internship Task 1: Data Cleaning & Preprocessing â€“ Titanic Dataset

## ðŸŽ¯ Objective

The objective of this task is to learn and demonstrate **data cleaning and preprocessing** techniques on a real-world dataset. We used the Titanic dataset to perform operations like null value handling, outlier treatment, categorical encoding, and feature scaling to prepare the data for machine learning models.

---

## ðŸ§° Tools & Libraries Used

- **Python**
- **Pandas** â€“ for data manipulation
- **NumPy** â€“ for numerical operations
- **Matplotlib / Seaborn** â€“ for data visualization
- **Scikit-learn** â€“ for feature scaling

---

## ðŸ“¥ Dataset Description

- Dataset Name: `Titanic-Dataset.csv`
- Shape: `891 rows Ã— 12 columns`
- Source: Loaded locally using `pandas.read_csv()`

---

## ðŸ§¾ Variable Interpretation

| Column       | Description |
|--------------|-------------|
| `PassengerId` | Unique ID assigned to each passenger |
| `Survived` | Survival status (0 = No, 1 = Yes) |
| `Pclass` | Passenger class (1 = Upper, 2 = Middle, 3 = Lower) |
| `Name` | Full name of the passenger |
| `Sex` | Gender |
| `Age` | Age in years (can be fractional for children) |
| `SibSp` | Number of siblings/spouses aboard |
| `Parch` | Number of parents/children aboard |
| `Ticket` | Ticket number |
| `Fare` | Ticket fare (in GBP) |
| `Cabin` | Cabin number (often missing) |
| `Embarked` | Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) |

---

## ðŸ“Š Step-by-Step Process

### ðŸ” Step 1: Data Overview

- Loaded the dataset using `pd.read_csv()`
- Explored structure using `.head()`, `.tail()`, `.info()`, `.describe()`
- Checked for nulls using `.isnull().sum()`

> **Observations:**
> - 891 total entries, 12 columns
> - Null values in `Age`, `Cabin`, and `Embarked`
> - Categorical columns: `Sex`, `Embarked`
> - Numerical columns: `Age`, `Fare`, `SibSp`, `Parch`, etc.

---

### ðŸ§¼ Step 2: Data Preprocessing

#### a) Data Cleaning
- Dropped irrelevant columns: `PassengerId`, `Name`, `Ticket`
- Renaming not needed as column names were already meaningful

#### b) Null Value Handling

##### ðŸ§¹ Manufacturing Rules:
1. **< 15% nulls** â†’ drop rows
2. **20%â€“70% nulls** â†’ impute
3. **> 75% nulls** â†’ drop column

##### ðŸ§  Imputation:
- Numerical (`Age`) â†’ filled with **mean**
- Categorical (`Embarked`) â†’ filled with **mode**
- `Cabin` column â†’ dropped (77% missing)

#### c) Outlier Treatment
- Visualized with boxplots (`Age`, `Fare`)
- Detected outliers treated using **IQR method**
- Outliers replaced with column mean

---

### ðŸ§¾ Step 3: Encoding Categorical Variables

- One-hot encoded `Sex` and `Embarked` using `pd.get_dummies()`
- Converted `True/False` to `1/0` using `.astype(int)`

---

### ðŸ“ Step 4: Feature Scaling

- Applied **StandardScaler** to normalize `Fare`
- Standardization not done on `Age` since some models (like trees) donâ€™t need it, but can be applied optionally

---

## ðŸ“ˆ Exploratory Data Analysis (EDA)

### ðŸ”— Feature Correlation

Generated a heatmap of feature correlations:
- `Fare` has a positive correlation (`0.28`) with `Survived`
- `Pclass` and `Fare` are negatively correlated (`-0.64`)
- `SibSp` and `Parch` moderately correlated (`0.41`)
- Weak or no correlation found in some features

---

## âœ… Final Cleaned Dataset

| Column | Description |
|--------|-------------|
| `Survived` | Target variable |
| `Pclass`, `Age`, `SibSp`, `Parch`, `Fare` | Scaled/cleaned numeric features |
| `Sex_male`, `Embarked_Q`, `Embarked_S` | One-hot encoded features |

All columns are now:
- Numeric
- Free of null values
- Scaled or encoded
- Ready for modeling

---

## ðŸ“Š Class Balance Check

Used `.value_counts(normalize=True)` on `Survived`:
- Class 0 (Did not survive): ~62%
- Class 1 (Survived): ~38%
> âœ… Not highly imbalanced â€“ safe for basic modeling without resampling

---

## ðŸ’¾ Output Files

- `Titanic_Cleaned.csv`: Cleaned dataset (optional to export)
- `Internship_task_1.ipynb`: Jupyter notebook with all code and steps
- `README.md`: This documentation file

---

## ðŸ“š Learnings

- How to explore and clean real-world data
- Null value handling strategies with practical rules
- Outlier visualization and treatment
- Categorical encoding and feature scaling
- Interpreting data distribution and correlation for ML readiness

---

## ðŸ“Œ Next Steps (Optional)

- Train classification models (Logistic Regression, Decision Trees, etc.)
- Perform univariate, bivariate, and multivariate EDA
- Feature selection using importance or RFE
- Build a prediction system with interpretability

---

## ðŸ§  Author Note

This task was completed as part of a **data science internship assignment**. The focus was on building a strong foundation in data preprocessing, which is the most critical stage before any ML model development.

---
